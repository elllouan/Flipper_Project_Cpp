A shader is a program that runs on the GPU. It is used to render shapes
out on the screen. There exist multiple shaders but here are the most common
ones:
- vertex shader: determines the position of each position vertex on the screen.
- geometry shader: 
- fragment shader: determines the color of each pixel on the shapes that was displayed.

The fragment shader is much more expensive operation because it gets called as many times
as the number of pixels there are within the shape defined by the vertex shader.

CHAP 1: OpenGL objects and graphics pipeline

The graphics pipeline is:
GPU -> vertex shader -> fragment shader -> screen

Some definitions:
- A fragment 
	is all the data required for OpenGL to render a single pixel.
- VBO for Vertex Buffer Object 
	is an OpenGL object that aims to store raw data such as position, color, textture, brightness ...
- VAO for Vertex Array Object 
	is an OpenGL object that aims to tell how to access the data of a bounded VBO.
https://www.youtube.com/watch?v=Ktp1P4J04Gw
- EBO for Element Buffer Object
	is a buffer, just like a VBO, that stores indices that OpenGL uses to decide what vertices to draw. It helps avoiding overlapping.


Steps to follow to render sth out:
Manage OpenGL objects:
	1. glGenBuffer or glGenVertexArrays: generate VBO, EBO or VAO.
	2. glBindBuffer or glBindVertexArray: bind VBO, EBO or VAOto the current context.
	3. glBufferData (for VBO and EBO): specify details about data for each buffer.
	4. glVertexAttribPointer: specify how to access attributes of the bounded VBO.
	5. glEnableVertexArray

Create simple shaders:
	1. 
	2.
	3.
	4.
	5.
	6.

Methods encountered:
glGen[Buffers/Arrays], glBind[Buffers/Arrays], glVertexAttribPointer, glEnableVertexAttribArray, glDraw[Buffers/Arrays],
glCreate[Shader/Program], glCompileShader, glAttachShader, glLinkProgram, glValidateProgram, glDelete[Buffers/Arrays/Shader],
glUseProgram, glDeleteProgram


CHAP 2: Shaders and uniforms with GLSL

	1. Transfer data from one shader to another one by using 'in' and 'out' keywords.
	2. Uniforms are another way to pass data from our application on the CPU to the shaders on the GPU.
		They are global variables that can be accesed from any shader and also from the code.
		Finding the uniform location does not require you to use the shader program first,
		but updating a uniform does require you to first use the program (by calling glUseProgram),
		because it sets the uniform on the currently active shader program.
	3. 

Methods encountered:
glDrawElements, glUniform, glGetUniformLocation, glUniform[1/2/3/4][f/i/b]


CHAP 3: Textures

	1. Texture wrapping: GL_[MIRRORED]_REPEAT / GL_CLAMP_TO_[EDGE/BORDER]
	2. Filtering methods: GL_[LINEAR/NEAREST] for magnifying (scaling upwards) or minifying (scaling downwards) a texture
	3. Mipmaps helps limiting the overhead of rendering small pieces of texture that are located next to bigger high resolution textures.
		It is a collection of texture images where each subsequent texture is twice as small compared to the previous one.
	4. Loading various type of images with stb_image.h
	5. Generating a texture
	6. Samplers are store textures that we load. They are used in the fragment shader to map the texture coordonates with our actual texture.
	7. Texture Units allow us to bind textures (or images) to a constant called GL_TEXTURE[0..15]. There 16 available with OpenGL.
		We do this by first activate the texture unit with glActiveTexture and we binf the desired texture to it afterwards.
		As a result, we can combine multiple textures (up to 16) within one vertex (with mix() in the fragment shader).

Methods encountered:
glTexParameter[f/i/iv/fv], glGenerateMipmap, glGenTextures, glBindTexture[s], glTexImage[1/2/3]D, glActiveTexture

IMPORTANT TO REMEMBER:
	- Bind (buffers, arrays, textures) first, use the shader program and modify uniform.


CHAP 4: Transformations

	1. Homogeneous coordinate is the 4th coordinate of a position vertex and is useful to perform translation using a matrix.
	2. Quaternions prevent from the Gimbal lock problem ...
	3. GLM for OpenGL Mathematics is a math library aimed at OpenGL: https://github.com/g-truc/glm/tree/master.
		Functions: glm::rotate(glm::mat4, glm::radians, glm::vec3), glm::translate(), glm::scale()
	4. How to assign to different textures to different elements? ==> Several calls of the glDrawElements function.

IMPORTANT TO REMEMBER:
	- Always printing a square centered and then apply transformations to it using matrices.
	- Between 2 transformations using matrices, do not forget to reset the matrix to the identity matrix.

CHAP 5: Coordinate systems

	1. Local space (or Object space)

	--> Model matrix
	2. World space: In this coordinate system all items are positionned relatively to one another using a reference point in the world space.

	--> View matrix
	3. View space (or camera): This coordinate system simulates the point of view of a camera/person.

	--> Projection matrix
	4. Clip space: In this coordinate system, items are projected in a space where coordinates extend from -1.0 to 1.0 for each axis (NDC for Normalized Device Coordinates).
		All coordinates out of that range get clipped and do not appear on the screen. We can use a projection matrix to map a more intuitive range of coordinates into these NDC.
		The matrix projection can be broken down into two separate actions:
			1. Orthographic projection: A container (or frustum) is specified by width, height and length. All coordinates outside the given ranges are discarded.
										This is not complete, though. We need to add perspective to our items.
			2. Perspective projection: This projection is performed with the homogeneous coordinate. 
										The projection matrix maps a given frustum range to clip space, but also manipulates the w value of each vertex coordinate
										in such a way that the further away a vertex coordinate is from the viewer, the higher this w component becomes.
										Once the coordinates are transformed to clip space they are in the range -w to w (anything outside this range is clipped). 
										OpenGL requires that the visible coordinates fall between the range -1.0 and 1.0 as the final vertex shader output,
										thus once the coordinates are in clip space, perspective division is applied to the clip space coordinatesand x, y and z coordinates are divided by w.
	
	5. Z-Buffer (or Depth buffer) allows for handling the depth of pixels within a frame. The depth is stored within each fragment.
		OpenGl determines which pixel should be rendered based off the depth of its fragment's z value.

Methods encountered:
gl[Enable/Disable], 